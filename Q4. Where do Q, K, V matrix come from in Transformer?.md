# Q4. 

## 1. Where do Q, K, V matrix come from in Transformer?

A: Q, K, V come from the Feed Forward Network in the Transformer, which usually have one hiddent layer larger than d_model.

The FFN has a structure like, d_embedding->d_ffn->d_model. In the Transformer the FFN are identical for different positions.

The Q, K, V are then projected as qs, ks, vs in multihead attention model with Wq_i, Wk_i, Wv_i matrixs
where i indicates the index of attention head.

## 2. Are Q, K, V identical for different layers?

A: No, they are different for different layers.


A good reference that explains Transformer quite well : 

https://jalammar.github.io/illustrated-transformer/
